{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83ed80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Document Structure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec80b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'exmaple.txt', 'pages': 1, 'author': 'Krish Naik', 'date_created': '2025-01-01'}, page_content='this is the main text content I am using to create RAG')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"this is the main text content I am using to create RAG\",\n",
    "    metadata={\n",
    "        \"source\":\"exmaple.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"PRATYAKSH SONI\",\n",
    "        \"date_created\":\"2026-01-13\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f852619",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a simple txt file\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb8a6db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"✅ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f4f9898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "### TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader(\"../data/text_files/python_intro.txt\",encoding=\"utf-8\")\n",
    "document=loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c06a4e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", ## Pattern to match files  \n",
    "    loader_cls= TextLoader, ##loader class to use\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75c2876f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'file_path': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': 'BFA PIPELINE', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='BFA Technical Specification: The \\nDual-Stream Feasibility Pipeline \\nProject Name: Business Feasibility Analyzer (BFA) \\nModule: Core Processing Pipeline v1.0 \\nArchitecture Type: Parallel Processing with Cross-Modal Validation \\n1. Executive Summary \\nThe BFA Pipeline is designed to automate the feasibility analysis of new business proposals. It \\nutilizes a Dual-Stream Architecture that separates unstructured semantic data (proposal \\ndocuments) from structured quantitative data (financials/projections). This separation allows for \\nspecialized processing of \"Intent\" versus \"Metrics\" before synthesizing them into a unified risk \\nassessment with visualized weak points and actionable feedback. \\n \\n2. High-Level Architecture Flow \\nThe pipeline operates in five distinct sequential stages: \\n1.\\u200b Dual-Input Ingestion: User submits qualitative and quantitative data separately. \\n2.\\u200b Parallel Pre-Processing: Simultaneous NLP parsing (Text) and Statistical Analysis \\n(Numeric). \\n3.\\u200b The Synthesis Core: Integration of user data with internal company databases \\n(HR/Finance) for validation. \\n4.\\u200b Logic & Visualization Engine: Detection of risk patterns (\"Falling Graphs\") and \\ngeneration of feedback. \\n5.\\u200b Dashboard Rendering: Final output display to the decision-maker. \\n \\n3. Detailed Stage Breakdown \\nStage 1: The Dual-Input Gateway (Frontend) \\nThe user interface is strictly divided to ensure data hygiene at the source. \\n●\\u200b Input Stream A: Context Layer (The \"Why\" & \"How\") \\n○\\u200b Data Type: Unstructured Document (PDF, DOCX, TXT). \\n○\\u200b Content: Executive Summary, Technical Architecture, Market Strategy, \\nOperational Plan. \\n○\\u200b Constraint: File size limit (e.g., 10MB), text-selectable format required. \\n●\\u200b Input Stream B: Metrics Layer (The \"What\") \\n○\\u200b Data Type: Structured Data (CSV upload or Interactive Grid).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'file_path': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': 'BFA PIPELINE', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='○\\u200b Content: \\n■\\u200b Budget: CAPEX, OPEX, Marketing spend. \\n■\\u200b Projections: 12-36 month Revenue, User Growth, ROI. \\n■\\u200b Resources: Headcount required, Time-to-Market (months). \\nStage 2: Parallel Pre-Processing \\nOnce submitted, the backend splits the workload into two processing tracks. \\nTrack A: Semantic Understanding (NLP) \\n●\\u200b Text Extraction: OCR/Parser removes formatting to extract raw text. \\n●\\u200b Entity Recognition (NER): Identifies key technologies, target demographics, and \\ncompetitors mentioned in the text. \\n●\\u200b Intent Parsing: Summarizes the \"Project Goal\" (e.g., “To launch a mobile-first SaaS \\nplatform in the APAC region”) to provide context for the numeric analysis. \\nTrack B: Numeric Normalization \\n●\\u200b Standardization: Converts all currency to the corporate base currency (e.g., USD) and \\naligns timelines to Fiscal Quarters (Q1-Q4). \\n●\\u200b Sanity Check: Validates that Revenue > 0, Cost > 0, and Profit = Revenue - Cost. \\nStage 3: The Integration & Validation Core \\nThis stage determines truth by comparing User Inputs against Internal Reality. \\n●\\u200b Internal Benchmarking (The \"Reality Check\"): \\n○\\u200b Input: User claims project takes 3 months. \\n○\\u200b Internal Data: Historical R&D data shows similar projects take 7 months. \\n○\\u200b Outcome: Flagged as \"Optimism Bias Risk.\" \\n●\\u200b Cross-Modal Validation: \\n○\\u200b Checks for consistency between the Document (Stream A) and the Numbers \\n(Stream B). \\n○\\u200b Example: Document mentions \"Aggressive Marketing Push\" but Spreadsheet \\nshows \"Zero Marketing Budget.\" \\n○\\u200b Outcome: Flagged as \"Strategic Mismatch.\" \\nStage 4: Visualization & Logic Engine \\nThis module generates the specific \"Weak Points\" analysis and graph data. \\n●\\u200b Trend Analysis (The \"Falling Graph\" Detector): \\n○\\u200b The engine calculates the derivative (slope) of the Revenue Projection curve. \\n○\\u200b Logic: If the slope becomes negative ($<0$) for two consecutive periods, or if \\nthe Net Profit margin dips below the company threshold (e.g., 10%), it marks \\nthose data points as Critical Weak Points. \\n●\\u200b Feedback Generation: \\n○\\u200b Constructs natural language sentences based on the flags.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'file_path': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': 'BFA PIPELINE', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='○\\u200b Positive: \"Budget allocation aligns with Q3 fiscal goals.\" \\n○\\u200b Negative: \"Projected growth creates a \\'Falling Graph\\' trajectory in Year 2; ROI is \\ninsufficient.\" \\nStage 5: Final Output Dashboard \\nThe user receives a structured JSON object rendered into a UI. \\n●\\u200b The Verdict: Feasible / Feasible with Adjustments / Not Feasible. \\n●\\u200b Interactive Graphs: \\n○\\u200b Revenue Projection: Line graph. Red zones highlight \"Falling\" periods. \\n○\\u200b Budget Burn: Bar chart. Shows projected spend vs. Company Cap. \\n●\\u200b Textual Feedback: Bulleted list of Pros and Cons derived from the analysis. \\n \\n4. Data Payload Specification (Sample Output) \\nThis is the JSON structure the backend sends to the frontend to visualize the results. \\nJSON \\n{ \\n  \"project_id\": \"BFA-2024-001\", \\n  \"feasibility_score\": \"Feasible with Adjustments\", \\n  \"score_confidence\": 0.85, \\n  \"analysis_summary\": { \\n    \"aim\": \"Launch AI-driven Logistics Platform\", \\n    \"market_fit\": \"High\", \\n    \"financial_risk\": \"Medium\" \\n  }, \\n  \"feedback\": { \\n    \"positive\": [ \\n      \"Technical stack aligns with current R&D capabilities.\", \\n      \"Initial 6-month growth projection is strong.\" \\n    ], \\n    \"negative\": [ \\n      \"Marketing budget is 40% below industry standard.\", \\n      \"Year 2 Q3 shows a significant revenue dip (Falling Graph).\" \\n    ] \\n  }, \\n  \"graph_data\": { \\n    \"projections\": [ \\n      {\"period\": \"Q1\", \"revenue\": 10000, \"status\": \"healthy\"}, \\n      {\"period\": \"Q2\", \"revenue\": 15000, \"status\": \"healthy\"}, \\n      {\"period\": \"Q3\", \"revenue\": 12000, \"status\": \"warning_falling\"}, \\n      {\"period\": \"Q4\", \"revenue\": 9000, \"status\": \"critical_falling\"} \\n    ],'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'file_path': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': 'BFA PIPELINE', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='\"weak_points\": [ \\n      { \\n        \"type\": \"Budget Mismatch\", \\n        \"description\": \"Proposed $10k marketing vs Required $50k\", \\n        \"severity\": \"High\" \\n      } \\n    ] \\n  } \\n} \\n \\n \\n5. Recommended Technology Stack \\nComponent \\nTechnology \\nFrontend \\nReact.js (for interactive Graphs via Recharts or Chart.js) \\nBackend API \\nPython (FastAPI or Django REST Framework) \\nDocument Parsing \\nApache Tika / PyMuPDF \\nNumeric Processing \\nPandas / NumPy \\nDatabase \\nPostgreSQL (Structured data), MongoDB (Logs/Documents) \\nSecurity \\nOAuth2.0, AES-256 Encryption for stored proposals')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\", ## Pattern to match files  \n",
    "    loader_cls= PyMuPDFLoader, ##loader class to use\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b479fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "###RAG Pipelines- Data Ingestion to Vector DB Pipeline\n",
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74175cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF files to process\n",
      "\n",
      "Processing: BFA PIPELINE .pdf\n",
      "  ✓ Loaded 4 pages\n",
      "\n",
      "Total documents loaded: 4\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b94c43ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='BFA  Technical  Specification:  The  \\nDual-Stream\\n \\nFeasibility\\n \\nPipeline\\n \\nProject  Name:  Business  Feasibility  Analyzer  (BFA)  \\nModule:  Core  Processing  Pipeline  v1.0  \\nArchitecture  Type:  Parallel  Processing  with  Cross-Modal  Validation  \\n1.  Executive  Summary  The  BFA  Pipeline  is  designed  to  automate  the  feasibility  analysis  of  new  business  proposals.  It  \\nutilizes\\n \\na\\n \\nDual-Stream\\n \\nArchitecture\\n \\nthat\\n \\nseparates\\n \\nunstructured\\n \\nsemantic\\n \\ndata\\n \\n(proposal\\n \\ndocuments)\\n \\nfrom\\n \\nstructured\\n \\nquantitative\\n \\ndata\\n \\n(financials/projections).\\n \\nThis\\n \\nseparation\\n \\nallows\\n \\nfor\\n \\nspecialized\\n \\nprocessing\\n \\nof\\n \\n\"Intent\"\\n \\nversus\\n \\n\"Metrics\"\\n \\nbefore\\n \\nsynthesizing\\n \\nthem\\n \\ninto\\n \\na\\n \\nunified\\n \\nrisk\\n \\nassessment\\n \\nwith\\n \\nvisualized\\n \\nweak\\n \\npoints\\n \\nand\\n \\nactionable\\n \\nfeedback.\\n \\n \\n2.  High-Level  Architecture  Flow  The  pipeline  operates  in  five  distinct  sequential  stages:  \\n1.  Dual-Input  Ingestion:  User  submits  qualitative  and  quantitative  data  separately.  2.  Parallel  Pre-Processing:  Simultaneous  NLP  parsing  (Text)  and  Statistical  Analysis  \\n(Numeric).\\n 3.  The  Synthesis  Core:  Integration  of  user  data  with  internal  company  databases  \\n(HR/Finance)\\n \\nfor\\n \\nvalidation.\\n 4.  Logic  &  Visualization  Engine:  Detection  of  risk  patterns  (\"Falling  Graphs\")  and  \\ngeneration\\n \\nof\\n \\nfeedback.\\n 5.  Dashboard  Rendering:  Final  output  display  to  the  decision-maker.  \\n \\n3.  Detailed  Stage  Breakdown  Stage  1:  The  Dual-Input  Gateway  (Frontend)  The  user  interface  is  strictly  divided  to  ensure  data  hygiene  at  the  source.  \\n●  Input  Stream  A:  Context  Layer  (The  \"Why\"  &  \"How\")  ○  Data  Type:  Unstructured  Document  (PDF,  DOCX,  TXT).  ○  Content:  Executive  Summary,  Technical  Architecture,  Market  Strategy,  \\nOperational\\n \\nPlan.\\n ○  Constraint:  File  size  limit  (e.g.,  10MB),  text-selectable  format  required.  ●  Input  Stream  B:  Metrics  Layer  (The  \"What\")  ○  Data  Type:  Structured  Data  (CSV  upload  or  Interactive  Grid).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='○  Content:  ■  Budget:  CAPEX,  OPEX,  Marketing  spend.  ■  Projections:  12-36  month  Revenue,  User  Growth,  ROI.  ■  Resources:  Headcount  required,  Time-to-Market  (months).  \\nStage  2:  Parallel  Pre-Processing  Once  submitted,  the  backend  splits  the  workload  into  two  processing  tracks.  \\nTrack  A:  Semantic  Understanding  (NLP)  ●  Text  Extraction:  OCR/Parser  removes  formatting  to  extract  raw  text.  ●  Entity  Recognition  (NER):  Identifies  key  technologies,  target  demographics,  and  \\ncompetitors\\n \\nmentioned\\n \\nin\\n \\nthe\\n \\ntext.\\n ●  Intent  Parsing:  Summarizes  the  \"Project  Goal\"  (e.g.,  “To  launch  a  mobile-first  SaaS  \\nplatform\\n \\nin\\n \\nthe\\n \\nAPAC\\n \\nregion”\\n)\\n \\nto\\n \\nprovide\\n \\ncontext\\n \\nfor\\n \\nthe\\n \\nnumeric\\n \\nanalysis.\\n \\nTrack  B:  Numeric  Normalization  ●  Standardization:  Converts  all  currency  to  the  corporate  base  currency  (e.g.,  USD)  and  \\naligns\\n \\ntimelines\\n \\nto\\n \\nFiscal\\n \\nQuarters\\n \\n(Q1-Q4).\\n ●  Sanity  Check:  Validates  that  Revenue  >  0,  Cost  >  0,  and  Profit  =  Revenue  -  Cost.  \\nStage  3:  The  Integration  &  Validation  Core  This  stage  determines  truth  by  comparing  User  Inputs  against  Internal  Reality.  \\n●  Internal  Benchmarking  (The  \"Reality  Check\"):  ○  Input:  User  claims  project  takes  3  months.  ○  Internal  Data:  Historical  R&D  data  shows  similar  projects  take  7  months.  ○  Outcome:  Flagged  as  \"Optimism  Bias  Risk.\"  ●  Cross-Modal  Validation:  ○  Checks  for  consistency  between  the  Document  (Stream  A)  and  the  Numbers  \\n(Stream\\n \\nB).\\n ○  Example:  Document  mentions  \"Aggressive  Marketing  Push\"  but  Spreadsheet  \\nshows\\n \\n\"Zero\\n \\nMarketing\\n \\nBudget.\"\\n ○  Outcome:  Flagged  as  \"Strategic  Mismatch.\"  \\nStage  4:  Visualization  &  Logic  Engine  This  module  generates  the  specific  \"Weak  Points\"  analysis  and  graph  data.  \\n●  Trend  Analysis  (The  \"Falling  Graph\"  Detector):  ○  The  engine  calculates  the  derivative  (slope)  of  the  Revenue  Projection  curve.  ○  Logic:  If  the  slope  becomes  negative  ($<0$)  for  two  consecutive  periods,  or  if  \\nthe\\n \\nNet\\n \\nProfit\\n \\nmargin\\n \\ndips\\n \\nbelow\\n \\nthe\\n \\ncompany\\n \\nthreshold\\n \\n(e.g.,\\n \\n10%),\\n \\nit\\n \\nmarks\\n \\nthose\\n \\ndata\\n \\npoints\\n \\nas\\n \\nCritical\\n \\nWeak\\n \\nPoints\\n.\\n ●  Feedback  Generation:  ○  Constructs  natural  language  sentences  based  on  the  flags.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 2, 'page_label': '3', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='○  Positive:  \"Budget  allocation  aligns  with  Q3  fiscal  goals.\"  ○  Negative:  \"Projected  growth  creates  a  \\'Falling  Graph\\'  trajectory  in  Year  2;  ROI  is  \\ninsufficient.\"\\n \\nStage  5:  Final  Output  Dashboard  The  user  receives  a  structured  JSON  object  rendered  into  a  UI.  \\n●  The  Verdict:  Feasible  /  Feasible  with  Adjustments  /  Not  Feasible.  ●  Interactive  Graphs:  ○  Revenue  Projection:  Line  graph.  Red  zones  highlight  \"Falling\"  periods.  ○  Budget  Burn:  Bar  chart.  Shows  projected  spend  vs.  Company  Cap.  ●  Textual  Feedback:  Bulleted  list  of  Pros  and  Cons  derived  from  the  analysis.  \\n \\n4.  Data  Payload  Specification  (Sample  Output)  This  is  the  JSON  structure  the  backend  sends  to  the  frontend  to  visualize  the  results.  \\nJSON  {    \"project_id\":  \"BFA-2024-001\",    \"feasibility_score\":  \"Feasible  with  Adjustments\",    \"score_confidence\":  0.85,    \"analysis_summary\":  {      \"aim\":  \"Launch  AI-driven  Logistics  Platform\",      \"market_fit\":  \"High\",      \"financial_risk\":  \"Medium\"    },    \"feedback\":  {      \"positive\":  [        \"Technical  stack  aligns  with  current  R&D  capabilities.\",        \"Initial  6-month  growth  projection  is  strong.\"      ],      \"negative\":  [        \"Marketing  budget  is  40%  below  industry  standard.\",        \"Year  2  Q3  shows  a  significant  revenue  dip  (Falling  Graph).\"      ]    },    \"graph_data\":  {      \"projections\":  [        {\"period\":  \"Q1\",  \"revenue\":  10000,  \"status\":  \"healthy\"},        {\"period\":  \"Q2\",  \"revenue\":  15000,  \"status\":  \"healthy\"},        {\"period\":  \"Q3\",  \"revenue\":  12000,  \"status\":  \"warning_falling\"},        {\"period\":  \"Q4\",  \"revenue\":  9000,  \"status\":  \"critical_falling\"}      ],'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 3, 'page_label': '4', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='\"weak_points\":  [        {          \"type\":  \"Budget  Mismatch\",          \"description\":  \"Proposed  $10k  marketing  vs  Required  $50k\",          \"severity\":  \"High\"        }      ]    }  }    \\n5.  Recommended  Technology  Stack  \\nComponent  Technology  \\nFrontend  React.js  (for  interactive  Graphs  via  Recharts  or  Chart.js)  \\nBackend  API  Python  (FastAPI  or  Django  REST  Framework)  \\nDocument  Parsing  Apache  Tika  /  PyMuPDF  \\nNumeric  Processing  Pandas  /  NumPy  \\nDatabase  PostgreSQL  (Structured  data),  MongoDB  (Logs/Documents)  \\nSecurity  OAuth2.0,  AES-256  Encryption  for  stored  proposals')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ea1a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c9a600b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 4 documents into 9 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: BFA  Technical  Specification:  The  \n",
      "Dual-Stream\n",
      " \n",
      "Feasibility\n",
      " \n",
      "Pipeline\n",
      " \n",
      "Project  Name:  Business  Feasibility  Analyzer  (BFA)  \n",
      "Module:  Core  Processing  Pipeline  v1.0  \n",
      "Architecture  Type:  P...\n",
      "Metadata: {'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='BFA  Technical  Specification:  The  \\nDual-Stream\\n \\nFeasibility\\n \\nPipeline\\n \\nProject  Name:  Business  Feasibility  Analyzer  (BFA)  \\nModule:  Core  Processing  Pipeline  v1.0  \\nArchitecture  Type:  Parallel  Processing  with  Cross-Modal  Validation  \\n1.  Executive  Summary  The  BFA  Pipeline  is  designed  to  automate  the  feasibility  analysis  of  new  business  proposals.  It  \\nutilizes\\n \\na\\n \\nDual-Stream\\n \\nArchitecture\\n \\nthat\\n \\nseparates\\n \\nunstructured\\n \\nsemantic\\n \\ndata\\n \\n(proposal\\n \\ndocuments)\\n \\nfrom\\n \\nstructured\\n \\nquantitative\\n \\ndata\\n \\n(financials/projections).\\n \\nThis\\n \\nseparation\\n \\nallows\\n \\nfor\\n \\nspecialized\\n \\nprocessing\\n \\nof\\n \\n\"Intent\"\\n \\nversus\\n \\n\"Metrics\"\\n \\nbefore\\n \\nsynthesizing\\n \\nthem\\n \\ninto\\n \\na\\n \\nunified\\n \\nrisk\\n \\nassessment\\n \\nwith\\n \\nvisualized\\n \\nweak\\n \\npoints\\n \\nand\\n \\nactionable\\n \\nfeedback.\\n \\n \\n2.  High-Level  Architecture  Flow  The  pipeline  operates  in  five  distinct  sequential  stages:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='risk\\n \\nassessment\\n \\nwith\\n \\nvisualized\\n \\nweak\\n \\npoints\\n \\nand\\n \\nactionable\\n \\nfeedback.\\n \\n \\n2.  High-Level  Architecture  Flow  The  pipeline  operates  in  five  distinct  sequential  stages:  \\n1.  Dual-Input  Ingestion:  User  submits  qualitative  and  quantitative  data  separately.  2.  Parallel  Pre-Processing:  Simultaneous  NLP  parsing  (Text)  and  Statistical  Analysis  \\n(Numeric).\\n 3.  The  Synthesis  Core:  Integration  of  user  data  with  internal  company  databases  \\n(HR/Finance)\\n \\nfor\\n \\nvalidation.\\n 4.  Logic  &  Visualization  Engine:  Detection  of  risk  patterns  (\"Falling  Graphs\")  and  \\ngeneration\\n \\nof\\n \\nfeedback.\\n 5.  Dashboard  Rendering:  Final  output  display  to  the  decision-maker.  \\n \\n3.  Detailed  Stage  Breakdown  Stage  1:  The  Dual-Input  Gateway  (Frontend)  The  user  interface  is  strictly  divided  to  ensure  data  hygiene  at  the  source.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='3.  Detailed  Stage  Breakdown  Stage  1:  The  Dual-Input  Gateway  (Frontend)  The  user  interface  is  strictly  divided  to  ensure  data  hygiene  at  the  source.  \\n●  Input  Stream  A:  Context  Layer  (The  \"Why\"  &  \"How\")  ○  Data  Type:  Unstructured  Document  (PDF,  DOCX,  TXT).  ○  Content:  Executive  Summary,  Technical  Architecture,  Market  Strategy,  \\nOperational\\n \\nPlan.\\n ○  Constraint:  File  size  limit  (e.g.,  10MB),  text-selectable  format  required.  ●  Input  Stream  B:  Metrics  Layer  (The  \"What\")  ○  Data  Type:  Structured  Data  (CSV  upload  or  Interactive  Grid).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='○  Content:  ■  Budget:  CAPEX,  OPEX,  Marketing  spend.  ■  Projections:  12-36  month  Revenue,  User  Growth,  ROI.  ■  Resources:  Headcount  required,  Time-to-Market  (months).  \\nStage  2:  Parallel  Pre-Processing  Once  submitted,  the  backend  splits  the  workload  into  two  processing  tracks.  \\nTrack  A:  Semantic  Understanding  (NLP)  ●  Text  Extraction:  OCR/Parser  removes  formatting  to  extract  raw  text.  ●  Entity  Recognition  (NER):  Identifies  key  technologies,  target  demographics,  and  \\ncompetitors\\n \\nmentioned\\n \\nin\\n \\nthe\\n \\ntext.\\n ●  Intent  Parsing:  Summarizes  the  \"Project  Goal\"  (e.g.,  “To  launch  a  mobile-first  SaaS  \\nplatform\\n \\nin\\n \\nthe\\n \\nAPAC\\n \\nregion”\\n)\\n \\nto\\n \\nprovide\\n \\ncontext\\n \\nfor\\n \\nthe\\n \\nnumeric\\n \\nanalysis.\\n \\nTrack  B:  Numeric  Normalization  ●  Standardization:  Converts  all  currency  to  the  corporate  base  currency  (e.g.,  USD)  and  \\naligns\\n \\ntimelines\\n \\nto\\n \\nFiscal\\n \\nQuarters\\n \\n(Q1-Q4).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='Track  B:  Numeric  Normalization  ●  Standardization:  Converts  all  currency  to  the  corporate  base  currency  (e.g.,  USD)  and  \\naligns\\n \\ntimelines\\n \\nto\\n \\nFiscal\\n \\nQuarters\\n \\n(Q1-Q4).\\n ●  Sanity  Check:  Validates  that  Revenue  >  0,  Cost  >  0,  and  Profit  =  Revenue  -  Cost.  \\nStage  3:  The  Integration  &  Validation  Core  This  stage  determines  truth  by  comparing  User  Inputs  against  Internal  Reality.  \\n●  Internal  Benchmarking  (The  \"Reality  Check\"):  ○  Input:  User  claims  project  takes  3  months.  ○  Internal  Data:  Historical  R&D  data  shows  similar  projects  take  7  months.  ○  Outcome:  Flagged  as  \"Optimism  Bias  Risk.\"  ●  Cross-Modal  Validation:  ○  Checks  for  consistency  between  the  Document  (Stream  A)  and  the  Numbers  \\n(Stream\\n \\nB).\\n ○  Example:  Document  mentions  \"Aggressive  Marketing  Push\"  but  Spreadsheet  \\nshows\\n \\n\"Zero\\n \\nMarketing\\n \\nBudget.\"\\n ○  Outcome:  Flagged  as  \"Strategic  Mismatch.\"'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='(Stream\\n \\nB).\\n ○  Example:  Document  mentions  \"Aggressive  Marketing  Push\"  but  Spreadsheet  \\nshows\\n \\n\"Zero\\n \\nMarketing\\n \\nBudget.\"\\n ○  Outcome:  Flagged  as  \"Strategic  Mismatch.\"  \\nStage  4:  Visualization  &  Logic  Engine  This  module  generates  the  specific  \"Weak  Points\"  analysis  and  graph  data.  \\n●  Trend  Analysis  (The  \"Falling  Graph\"  Detector):  ○  The  engine  calculates  the  derivative  (slope)  of  the  Revenue  Projection  curve.  ○  Logic:  If  the  slope  becomes  negative  ($<0$)  for  two  consecutive  periods,  or  if  \\nthe\\n \\nNet\\n \\nProfit\\n \\nmargin\\n \\ndips\\n \\nbelow\\n \\nthe\\n \\ncompany\\n \\nthreshold\\n \\n(e.g.,\\n \\n10%),\\n \\nit\\n \\nmarks\\n \\nthose\\n \\ndata\\n \\npoints\\n \\nas\\n \\nCritical\\n \\nWeak\\n \\nPoints\\n.\\n ●  Feedback  Generation:  ○  Constructs  natural  language  sentences  based  on  the  flags.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 2, 'page_label': '3', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='○  Positive:  \"Budget  allocation  aligns  with  Q3  fiscal  goals.\"  ○  Negative:  \"Projected  growth  creates  a  \\'Falling  Graph\\'  trajectory  in  Year  2;  ROI  is  \\ninsufficient.\"\\n \\nStage  5:  Final  Output  Dashboard  The  user  receives  a  structured  JSON  object  rendered  into  a  UI.  \\n●  The  Verdict:  Feasible  /  Feasible  with  Adjustments  /  Not  Feasible.  ●  Interactive  Graphs:  ○  Revenue  Projection:  Line  graph.  Red  zones  highlight  \"Falling\"  periods.  ○  Budget  Burn:  Bar  chart.  Shows  projected  spend  vs.  Company  Cap.  ●  Textual  Feedback:  Bulleted  list  of  Pros  and  Cons  derived  from  the  analysis.  \\n \\n4.  Data  Payload  Specification  (Sample  Output)  This  is  the  JSON  structure  the  backend  sends  to  the  frontend  to  visualize  the  results.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 2, 'page_label': '3', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='JSON  {    \"project_id\":  \"BFA-2024-001\",    \"feasibility_score\":  \"Feasible  with  Adjustments\",    \"score_confidence\":  0.85,    \"analysis_summary\":  {      \"aim\":  \"Launch  AI-driven  Logistics  Platform\",      \"market_fit\":  \"High\",      \"financial_risk\":  \"Medium\"    },    \"feedback\":  {      \"positive\":  [        \"Technical  stack  aligns  with  current  R&D  capabilities.\",        \"Initial  6-month  growth  projection  is  strong.\"      ],      \"negative\":  [        \"Marketing  budget  is  40%  below  industry  standard.\",        \"Year  2  Q3  shows  a  significant  revenue  dip  (Falling  Graph).\"      ]    },    \"graph_data\":  {      \"projections\":  [        {\"period\":  \"Q1\",  \"revenue\":  10000,  \"status\":  \"healthy\"},        {\"period\":  \"Q2\",  \"revenue\":  15000,  \"status\":  \"healthy\"},        {\"period\":  \"Q3\",  \"revenue\":  12000,  \"status\":  \"warning_falling\"},        {\"period\":  \"Q4\",  \"revenue\":  9000,  \"status\":  \"critical_falling\"}      ],'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 3, 'page_label': '4', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='\"weak_points\":  [        {          \"type\":  \"Budget  Mismatch\",          \"description\":  \"Proposed  $10k  marketing  vs  Required  $50k\",          \"severity\":  \"High\"        }      ]    }  }    \\n5.  Recommended  Technology  Stack  \\nComponent  Technology  \\nFrontend  React.js  (for  interactive  Graphs  via  Recharts  or  Chart.js)  \\nBackend  API  Python  (FastAPI  or  Django  REST  Framework)  \\nDocument  Parsing  Apache  Tika  /  PyMuPDF  \\nNumeric  Processing  Pandas  /  NumPy  \\nDatabase  PostgreSQL  (Structured  data),  MongoDB  (Logs/Documents)  \\nSecurity  OAuth2.0,  AES-256  Encryption  for  stored  proposals')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ade9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### embedding and vectorestoreDB\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98136867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Pratyaksh Personal\\My Projects\\RAGtest\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Pratyaksh Soni\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x2a97fb7d160>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce040729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from typing import List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3e126af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x2a92bcd6ba0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VectorStore\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "196b678b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='BFA  Technical  Specification:  The  \\nDual-Stream\\n \\nFeasibility\\n \\nPipeline\\n \\nProject  Name:  Business  Feasibility  Analyzer  (BFA)  \\nModule:  Core  Processing  Pipeline  v1.0  \\nArchitecture  Type:  Parallel  Processing  with  Cross-Modal  Validation  \\n1.  Executive  Summary  The  BFA  Pipeline  is  designed  to  automate  the  feasibility  analysis  of  new  business  proposals.  It  \\nutilizes\\n \\na\\n \\nDual-Stream\\n \\nArchitecture\\n \\nthat\\n \\nseparates\\n \\nunstructured\\n \\nsemantic\\n \\ndata\\n \\n(proposal\\n \\ndocuments)\\n \\nfrom\\n \\nstructured\\n \\nquantitative\\n \\ndata\\n \\n(financials/projections).\\n \\nThis\\n \\nseparation\\n \\nallows\\n \\nfor\\n \\nspecialized\\n \\nprocessing\\n \\nof\\n \\n\"Intent\"\\n \\nversus\\n \\n\"Metrics\"\\n \\nbefore\\n \\nsynthesizing\\n \\nthem\\n \\ninto\\n \\na\\n \\nunified\\n \\nrisk\\n \\nassessment\\n \\nwith\\n \\nvisualized\\n \\nweak\\n \\npoints\\n \\nand\\n \\nactionable\\n \\nfeedback.\\n \\n \\n2.  High-Level  Architecture  Flow  The  pipeline  operates  in  five  distinct  sequential  stages:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='risk\\n \\nassessment\\n \\nwith\\n \\nvisualized\\n \\nweak\\n \\npoints\\n \\nand\\n \\nactionable\\n \\nfeedback.\\n \\n \\n2.  High-Level  Architecture  Flow  The  pipeline  operates  in  five  distinct  sequential  stages:  \\n1.  Dual-Input  Ingestion:  User  submits  qualitative  and  quantitative  data  separately.  2.  Parallel  Pre-Processing:  Simultaneous  NLP  parsing  (Text)  and  Statistical  Analysis  \\n(Numeric).\\n 3.  The  Synthesis  Core:  Integration  of  user  data  with  internal  company  databases  \\n(HR/Finance)\\n \\nfor\\n \\nvalidation.\\n 4.  Logic  &  Visualization  Engine:  Detection  of  risk  patterns  (\"Falling  Graphs\")  and  \\ngeneration\\n \\nof\\n \\nfeedback.\\n 5.  Dashboard  Rendering:  Final  output  display  to  the  decision-maker.  \\n \\n3.  Detailed  Stage  Breakdown  Stage  1:  The  Dual-Input  Gateway  (Frontend)  The  user  interface  is  strictly  divided  to  ensure  data  hygiene  at  the  source.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='3.  Detailed  Stage  Breakdown  Stage  1:  The  Dual-Input  Gateway  (Frontend)  The  user  interface  is  strictly  divided  to  ensure  data  hygiene  at  the  source.  \\n●  Input  Stream  A:  Context  Layer  (The  \"Why\"  &  \"How\")  ○  Data  Type:  Unstructured  Document  (PDF,  DOCX,  TXT).  ○  Content:  Executive  Summary,  Technical  Architecture,  Market  Strategy,  \\nOperational\\n \\nPlan.\\n ○  Constraint:  File  size  limit  (e.g.,  10MB),  text-selectable  format  required.  ●  Input  Stream  B:  Metrics  Layer  (The  \"What\")  ○  Data  Type:  Structured  Data  (CSV  upload  or  Interactive  Grid).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='○  Content:  ■  Budget:  CAPEX,  OPEX,  Marketing  spend.  ■  Projections:  12-36  month  Revenue,  User  Growth,  ROI.  ■  Resources:  Headcount  required,  Time-to-Market  (months).  \\nStage  2:  Parallel  Pre-Processing  Once  submitted,  the  backend  splits  the  workload  into  two  processing  tracks.  \\nTrack  A:  Semantic  Understanding  (NLP)  ●  Text  Extraction:  OCR/Parser  removes  formatting  to  extract  raw  text.  ●  Entity  Recognition  (NER):  Identifies  key  technologies,  target  demographics,  and  \\ncompetitors\\n \\nmentioned\\n \\nin\\n \\nthe\\n \\ntext.\\n ●  Intent  Parsing:  Summarizes  the  \"Project  Goal\"  (e.g.,  “To  launch  a  mobile-first  SaaS  \\nplatform\\n \\nin\\n \\nthe\\n \\nAPAC\\n \\nregion”\\n)\\n \\nto\\n \\nprovide\\n \\ncontext\\n \\nfor\\n \\nthe\\n \\nnumeric\\n \\nanalysis.\\n \\nTrack  B:  Numeric  Normalization  ●  Standardization:  Converts  all  currency  to  the  corporate  base  currency  (e.g.,  USD)  and  \\naligns\\n \\ntimelines\\n \\nto\\n \\nFiscal\\n \\nQuarters\\n \\n(Q1-Q4).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='Track  B:  Numeric  Normalization  ●  Standardization:  Converts  all  currency  to  the  corporate  base  currency  (e.g.,  USD)  and  \\naligns\\n \\ntimelines\\n \\nto\\n \\nFiscal\\n \\nQuarters\\n \\n(Q1-Q4).\\n ●  Sanity  Check:  Validates  that  Revenue  >  0,  Cost  >  0,  and  Profit  =  Revenue  -  Cost.  \\nStage  3:  The  Integration  &  Validation  Core  This  stage  determines  truth  by  comparing  User  Inputs  against  Internal  Reality.  \\n●  Internal  Benchmarking  (The  \"Reality  Check\"):  ○  Input:  User  claims  project  takes  3  months.  ○  Internal  Data:  Historical  R&D  data  shows  similar  projects  take  7  months.  ○  Outcome:  Flagged  as  \"Optimism  Bias  Risk.\"  ●  Cross-Modal  Validation:  ○  Checks  for  consistency  between  the  Document  (Stream  A)  and  the  Numbers  \\n(Stream\\n \\nB).\\n ○  Example:  Document  mentions  \"Aggressive  Marketing  Push\"  but  Spreadsheet  \\nshows\\n \\n\"Zero\\n \\nMarketing\\n \\nBudget.\"\\n ○  Outcome:  Flagged  as  \"Strategic  Mismatch.\"'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='(Stream\\n \\nB).\\n ○  Example:  Document  mentions  \"Aggressive  Marketing  Push\"  but  Spreadsheet  \\nshows\\n \\n\"Zero\\n \\nMarketing\\n \\nBudget.\"\\n ○  Outcome:  Flagged  as  \"Strategic  Mismatch.\"  \\nStage  4:  Visualization  &  Logic  Engine  This  module  generates  the  specific  \"Weak  Points\"  analysis  and  graph  data.  \\n●  Trend  Analysis  (The  \"Falling  Graph\"  Detector):  ○  The  engine  calculates  the  derivative  (slope)  of  the  Revenue  Projection  curve.  ○  Logic:  If  the  slope  becomes  negative  ($<0$)  for  two  consecutive  periods,  or  if  \\nthe\\n \\nNet\\n \\nProfit\\n \\nmargin\\n \\ndips\\n \\nbelow\\n \\nthe\\n \\ncompany\\n \\nthreshold\\n \\n(e.g.,\\n \\n10%),\\n \\nit\\n \\nmarks\\n \\nthose\\n \\ndata\\n \\npoints\\n \\nas\\n \\nCritical\\n \\nWeak\\n \\nPoints\\n.\\n ●  Feedback  Generation:  ○  Constructs  natural  language  sentences  based  on  the  flags.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 2, 'page_label': '3', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='○  Positive:  \"Budget  allocation  aligns  with  Q3  fiscal  goals.\"  ○  Negative:  \"Projected  growth  creates  a  \\'Falling  Graph\\'  trajectory  in  Year  2;  ROI  is  \\ninsufficient.\"\\n \\nStage  5:  Final  Output  Dashboard  The  user  receives  a  structured  JSON  object  rendered  into  a  UI.  \\n●  The  Verdict:  Feasible  /  Feasible  with  Adjustments  /  Not  Feasible.  ●  Interactive  Graphs:  ○  Revenue  Projection:  Line  graph.  Red  zones  highlight  \"Falling\"  periods.  ○  Budget  Burn:  Bar  chart.  Shows  projected  spend  vs.  Company  Cap.  ●  Textual  Feedback:  Bulleted  list  of  Pros  and  Cons  derived  from  the  analysis.  \\n \\n4.  Data  Payload  Specification  (Sample  Output)  This  is  the  JSON  structure  the  backend  sends  to  the  frontend  to  visualize  the  results.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 2, 'page_label': '3', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='JSON  {    \"project_id\":  \"BFA-2024-001\",    \"feasibility_score\":  \"Feasible  with  Adjustments\",    \"score_confidence\":  0.85,    \"analysis_summary\":  {      \"aim\":  \"Launch  AI-driven  Logistics  Platform\",      \"market_fit\":  \"High\",      \"financial_risk\":  \"Medium\"    },    \"feedback\":  {      \"positive\":  [        \"Technical  stack  aligns  with  current  R&D  capabilities.\",        \"Initial  6-month  growth  projection  is  strong.\"      ],      \"negative\":  [        \"Marketing  budget  is  40%  below  industry  standard.\",        \"Year  2  Q3  shows  a  significant  revenue  dip  (Falling  Graph).\"      ]    },    \"graph_data\":  {      \"projections\":  [        {\"period\":  \"Q1\",  \"revenue\":  10000,  \"status\":  \"healthy\"},        {\"period\":  \"Q2\",  \"revenue\":  15000,  \"status\":  \"healthy\"},        {\"period\":  \"Q3\",  \"revenue\":  12000,  \"status\":  \"warning_falling\"},        {\"period\":  \"Q4\",  \"revenue\":  9000,  \"status\":  \"critical_falling\"}      ],'),\n",
       " Document(metadata={'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'BFA PIPELINE', 'source': '..\\\\data\\\\pdf\\\\BFA PIPELINE .pdf', 'total_pages': 4, 'page': 3, 'page_label': '4', 'source_file': 'BFA PIPELINE .pdf', 'file_type': 'pdf'}, page_content='\"weak_points\":  [        {          \"type\":  \"Budget  Mismatch\",          \"description\":  \"Proposed  $10k  marketing  vs  Required  $50k\",          \"severity\":  \"High\"        }      ]    }  }    \\n5.  Recommended  Technology  Stack  \\nComponent  Technology  \\nFrontend  React.js  (for  interactive  Graphs  via  Recharts  or  Chart.js)  \\nBackend  API  Python  (FastAPI  or  Django  REST  Framework)  \\nDocument  Parsing  Apache  Tika  /  PyMuPDF  \\nNumeric  Processing  Pandas  /  NumPy  \\nDatabase  PostgreSQL  (Structured  data),  MongoDB  (Logs/Documents)  \\nSecurity  OAuth2.0,  AES-256  Encryption  for  stored  proposals')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81d3cba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 9 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (9, 384)\n",
      "Adding 9 documents to vector store...\n",
      "Successfully added 9 documents to vector store\n",
      "Total documents in collection: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "163a16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Retriever Pipeline From VectorStore\n",
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ca35ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x2a92c425400>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd943fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is buissness feasebility analyzer?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 52.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " rag_retriever.retrieve(\"What is buissness feasebility analyzer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e11da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Project complete basic rag retriever and embedding model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGtest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
